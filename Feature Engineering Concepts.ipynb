{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b30a70-dd46-4938-bcf4-7584cd2ff4ff",
   "metadata": {},
   "source": [
    "Missing Values- Feature Engineering- Day 1\n",
    "Lifecycle of a Data Science Projects\n",
    "\n",
    "Data Collection Statergy---from company side,3rd party APi's,Surveys\n",
    "Feature Engineering---Handling Missing Values\n",
    "Why are their Missing values?? Survey--Depression Survey\n",
    "\n",
    "They hesitate to put down the information\n",
    "Survey informations are not that valid\n",
    "Men hide their salary\n",
    "Women hide their age\n",
    "People may have died----NAN\n",
    "There are two type of missing data.\n",
    "\n",
    "continuous/ Discrete\n",
    "Categorical\n",
    "Data Science Projects---Dataset should be collected from multiple sources\n",
    "\n",
    "What are the different types of Missing Data?\n",
    "Missing Completely at Random (MCAR):\n",
    "A variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations. When data is MCAR, there is absolutely no relationship between the data missing and any other values, observed or missing, within the dataset. In other words, those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than other.\n",
    "All the techniques of handling ,missing values\n",
    "Mean/ Median/Mode replacement (Famous)\n",
    "Random Sample Imputation (Famous)\n",
    "Capturing NAN values with a new feature\n",
    "End of Distribution imputation\n",
    "Arbitrary imputation\n",
    "Frequent categories imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac99af-9949-4b49-9f62-06eb9702d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cf470-1986-448b-a62a-b837a71c0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dba3a-3db2-4766-8fab-76e618dcf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf69f2-6de9-4343-8e42-098154dec690",
   "metadata": {},
   "source": [
    "Missing Data Not At Random(MNAR): Systematic missing Values\n",
    "There is absolutely some relationship between the data missing and any other values, observed or missing, within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d0136-a9b7-42b8-a201-c73329e451fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['cabin_null']=np.where(df['Cabin'].isnull(),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53c568-4847-42f4-90f1-b56e817e32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##find the percentage of null values\n",
    "df['cabin_null'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bcf5c-d2ce-47c0-97a8-47144749c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2c567-c53f-40d1-96f0-62c824c5a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Survived'])['cabin_null'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca6a48-2bb9-4d7d-b5e4-4e204580be0c",
   "metadata": {},
   "source": [
    "Missing At Random(MAR)\n",
    "Men---hide their salary\n",
    "Women---hide their age\n",
    "All the techniques of handling ,missing values\n",
    "Mean/ Median/Mode replacement (Famous)\n",
    "Random Sample Imputation (Famous)\n",
    "Capturing NAN values with a new feature\n",
    "End of Distribution imputation\n",
    "Arbitrary imputation\n",
    "Frequent categories imputation\n",
    "1. Mean/ MEdian /Mode imputation (Famous)\n",
    "When should we apply? Mean/median imputation has the assumption that the data are missing completely at random(MCAR). We solve this by replacing the NAN with the most frequent occurance of the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421719a8-5ad7-40e5-87ae-da913fbf374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('titanic.csv',usecols=['Age','Fare','Survived'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f99f47-c7b1-45b8-87ab-ad0b71c642ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets go and see the percentage of missing values\n",
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ff5d4-eef7-485f-a1e8-eed9e2853291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df,variable,median):\n",
    "    df[variable+\"_median\"]=df[variable].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703587a3-009c-4372-a578-c3245c27795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "median=df.Age.median()\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ba1e6-ac40-4fff-bf4d-2af0f53e82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_nan(df,'Age',median)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749415e6-6ac4-434c-b8d1-4f2ed5d86e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing standard deviation of Age and Age_median\n",
    "print(df['Age'].std())\n",
    "print(df['Age_median'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f2369-a5a8-4fc7-878a-85d82474c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "df['Age'].plot(kind='kde', ax=ax)\n",
    "df.Age_median.plot(kind='kde', ax=ax, color='red')\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dbe0e9-06fc-49ae-9d78-658e9dbdb682",
   "metadata": {},
   "source": [
    "Advantages And Disadvantages of Mean/Median Imputation\n",
    "Advantages\n",
    "Easy to implement(Robust to outliers)\n",
    "Faster way to obtain the complete dataset\n",
    "Disadvantages\n",
    "Change or Distortion in the original variance\n",
    "Impacts Correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eaeceb-6a95-4f52-8067-2c3e303bbf0c",
   "metadata": {},
   "source": [
    "2. Random Sample Imputation (Famous)\n",
    "Aim: Random sample imputation consists of taking random observation from the dataset and we use this observation to replace the nan values\n",
    "\n",
    "When should it be used? It assumes that the data are missing completely at random(MCAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60515310-9b96-4f09-b925-d0e76fe20922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4325ff-e2e6-4725-a81f-06ac93c0c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996456c4-ab36-4d83-9184-b7d9a6b60450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d13eb-38c5-4a09-9666-b3d81d8c6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51467f59-a5db-41e1-87ac-4799cab05bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping nan value and collecting some random sample of the data\n",
    "df['Age'].dropna().sample(df['Age'].isnull().sum(),random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc83a3-b335-40f1-9151-0b4dd399b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values index\n",
    "df[df['Age'].isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712bd17-f965-404b-aa21-7a064635b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df,variable,median):\n",
    "    df[variable+\"_median\"]=df[variable].fillna(median) #For comparison\n",
    "    df[variable+\"_random\"]=df[variable]\n",
    "    ##It will have the random sample to fill the na\n",
    "    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n",
    "    ##pandas needs to have same index in order to merge the dataset\n",
    "    #Replacing index of random_sample with null value's index\n",
    "    random_sample.index=df[df[variable].isnull()].index\n",
    "    df.loc[df[variable].isnull(),variable+'_random']=random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90709c-0ad3-4dc7-bbeb-9ff4d3534794",
   "metadata": {},
   "outputs": [],
   "source": [
    "median=df.Age.median()\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756499b3-cfdc-4ab7-911c-ed81125ce37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_nan(df,\"Age\",median) #Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d856ccc-584d-4389-ac53-d8150432ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad02e51-0d27-480f-b517-9fd55d27a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579a7f0-1405-42bd-a75b-051c5e768dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "df['Age'].plot(kind='kde', ax=ax)\n",
    "df.Age_median.plot(kind='kde', ax=ax, color='red')\n",
    "df.Age_random.plot(kind='kde', ax=ax, color='green')\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e3ca0-f488-44ef-b22d-03cfcd35868d",
   "metadata": {},
   "source": [
    "\n",
    "Advantages\n",
    "Easy To implement\n",
    "There is less distortion in variance\n",
    "Disadvantage\n",
    "Every situation randomness wont work\n",
    "3. Capturing NAN values with a new feature\n",
    "It works well if the data are not missing completely at random .. Here we use importance of missing value . in importance of the missing value we have to replace the nan value by ther median as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccae30e-0037-4ff4-b325-22c40ae3c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c38d0b-b7c0-4592-b8d3-1db66c4f1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Creating new feature nan value\n",
    "df['Age_NAN']=np.where(df['Age'].isnull(),1,0) #Importance of missing value\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b377d3-e3db-4ce0-81f1-8eb508f93ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0251a04-9f1d-4c53-a68b-7e6d3a72dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].fillna(df.Age.median(),inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd11f1f-90b6-43dd-be1d-d01537b49968",
   "metadata": {},
   "source": [
    "Advantages\n",
    "Easy to implement\n",
    "Captures the importance of missing values\n",
    "Disadvantages\n",
    "Creating Additional Features(Curse of Dimensionality)\n",
    "4. End of Distribution imputation\n",
    "It works well if the data are not missing completely at random. If there is suspicion that the missing value is not at random then capturing that information is important. In this scenario, one would want to replace missing data with values that are at the tails of the distribution of the variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db40472-5f06-43c4-9606-b9c8b7e5ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('titanic.csv', usecols=['Age','Fare','Survived'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a185cd-0a35-4ca5-81d5-0a4ed7f3a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7f237-0e1e-469c-8136-96f99b19d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of distribution\n",
    "extreme=df.Age.mean()+3*df.Age.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64a4e0-6ed7-4f32-85bd-ceda60d0b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot('Age',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746aab9-79d2-49b7-8d4b-dbd10a7daaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df,variable,median,extreme):\n",
    "    df[variable+\"_end_distribution\"]=df[variable].fillna(extreme)\n",
    "    df[variable].fillna(median,inplace=True) #for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac124a7-61be-4a8c-bb35-28e22355a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_nan(df,'Age',df.Age.median(),extreme)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3da594-682c-4bf3-af84-90f3f4778499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df0bb9-ed82-4624-bd23-62c6c8ddee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_end_distribution'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b410c-4eb1-41bb-9855-f450283aa3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot('Age_end_distribution',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d1a20-9f81-43ff-9768-f577f90d7ab1",
   "metadata": {},
   "source": [
    "Advantages\n",
    "Easy to implement\n",
    "Capture the importance of missingness if there is one\n",
    "Disadvantages\n",
    "Distorts the original distribution of variable\n",
    "if missingness is not important, It may mask the predictive power of original variable by distoring its distribution\n",
    "If the number of NA is big, It will mask outlier in distribution\n",
    "If the number of NA is small, The replaced Na maybe considered an outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7b802-262e-4f35-a391-527fd581f168",
   "metadata": {},
   "source": [
    "5. Arbitrary Value Imputation (It is not much uses )\n",
    "This technique was derived from kaggle competition It consists of replacing NAN by an arbitrary value.. (It is not much uses )\n",
    "\n",
    "When to use arbitrary value imputation:\n",
    "Replacing the NA by arbitrary values should be used when there are reasons to believe that the NA is not missing at random. In situations like this, we would not like to replace it with the median or the mean and therefore make the NA look like the majority of our observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51184190-0c71-48da-b1f0-5dcbe4da59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"titanic.csv\", usecols=[\"Age\",\"Fare\",\"Survived\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19b5bf-3197-40d3-bf59-e2ec69363321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we should use the lowest or highedt value / outlier like 0/80.. but i have used 100 \n",
    "def impute_nan(df,variable):\n",
    "    df[variable+'_zero']=df[variable].fillna(0)  # 0 either 100\n",
    "    df[variable+'_hundred']=df[variable].fillna(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659f67b-3023-4980-9249-429f49481dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_nan(df,\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084de91f-fd6e-488a-b6f5-9c2e453511ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36228d-64d0-4c86-854a-38d387b4ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2a184-ebbd-402c-abfc-6a8c494cb0f8",
   "metadata": {},
   "source": [
    "Advantages\n",
    "Easy to implement\n",
    "Captures the importance of missingess if there is one\n",
    "Disadvantages\n",
    "Distorts the original distribution of the variable\n",
    "If missingess is not important, it may mask the predictive power of the original variable by distorting its distribution\n",
    "Hard to decide which value to use\n",
    "How To Handle Categroical Missing Values\n",
    "6. Frequent Category Imputation\n",
    "You can use this method when data is missing completely at random, and no more than 5% of the variable contains missing data. It means we should use this method when data are missig very less like 40%(assuming) of missing value.In this method we fill the NAN value with most frequent occuring category in the data set.But if we have a lots of missing value in a feature then it won't work efficiently. Here there are a lots of missing value in \"FireplaceQu\" feature. so, this method won't work well on this feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807ab91-458c-4080-8c80-f1a07f07b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('loan.csv') # Here loan.csv is the Advance house pricing data set \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f8de6-efc5-4b34-b0ce-54f8b0df2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709e80a-8487-4669-881b-b2487ef4c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('loan.csv', usecols=['BsmtQual','FireplaceQu','GarageType','SalePrice'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4822871-edf3-4a7a-8130-8e6a129f9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac057da3-4b15-4570-9a76-dc29aa412360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa214fb-a2b5-465c-acfb-1925e10b8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1f40a-5c2e-4a35-bca3-82e2df3dbacd",
   "metadata": {},
   "source": [
    "Compute the frequency with every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245cc7e-a47c-4b12-8766-6e4917742437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['BsmtQual'])['BsmtQual'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64174e7f-e9e9-46a3-a59b-1be8d534e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtQual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ee631-552d-454a-9c3e-e88ce98bf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtQual'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319fb73-96b8-4974-bac2-debe761173af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['BsmtQual'])['BsmtQual'].count().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc92d51-dc02-4f86-9021-e36ecbc46ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GarageType'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088de75-0165-47b8-82c3-5af9557421a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FireplaceQu'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3dc60-9730-4824-a2e5-3dff365058f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GarageType'].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d938b4e-9a30-4543-b70f-b859f3087a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GarageType'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac41d5f-4c89-41e7-9a39-f7c605e6c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df,variable):\n",
    "    most_frequent_category=df[variable].mode()[0]\n",
    "    df[variable].fillna(most_frequent_category,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4eb11-25ad-4e1e-ac87-b9bc87bb26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['BsmtQual','FireplaceQu','GarageType']:\n",
    "    impute_nan(df,feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ad750-c403-4fbd-a03e-e9dce3bed00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd582fb-8ee3-4958-880a-727a7536a700",
   "metadata": {},
   "source": [
    "Advantages\n",
    "Easy To implement\n",
    "Faster way to implement\n",
    "Disadvantages\n",
    "Since we are using the more frequent labels, it may use them in an over respresented way, if there are many nan's\n",
    "It distorts the relation of the most frequent label\n",
    "* Adding a variable to capture NAN\n",
    "It works well if the data are not missing completely at random .. Here we use importance of missing value by creating a new feature and also fill the nan value by the most frequent occuring category in the perticuler variable. In this method if we have a lots of missing value in a feature then it will work well on this perticuler variable, In this data set there are a lots of missing value in \"FireplaceQu\" feature. so it will work well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a49c3d-9492-43d1-ae5c-7e1808783b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('loan.csv', usecols=['BsmtQual','FireplaceQu','GarageType','SalePrice'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a0c5a-b97a-4abe-b443-07186a8762cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['BsmtQual_Var']=np.where(df['BsmtQual'].isnull(),1,0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f28633-1f61-4c09-a274-1b5b7b22fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent = df['BsmtQual'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15011aed-949d-40a7-a44d-ea2ba1060b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtQual'].fillna(frequent,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7683987-b2f3-4b32-9ee7-03070508ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FireplaceQu_Var']=np.where(df['FireplaceQu'].isnull(),1,0)\n",
    "frequent=df['FireplaceQu'].mode()[0]\n",
    "df['FireplaceQu'].fillna(frequent,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75cfbb-186d-4cf8-8287-9d755af3b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb42fd-a0f4-489a-857e-0950709f84d7",
   "metadata": {},
   "source": [
    "* Suppose if you have more frequent categories, we just replace NAN with a new category (This technique uses most frequent)\n",
    "and it has lots of advantages. This is the sub category of \"Adding a variable to capture NAN\" method's. You can use either between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29954d49-5c86-45b4-9466-1f2b48f556f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('loan.csv', usecols=['BsmtQual','FireplaceQu','GarageType','SalePrice'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac5e4f-be4b-4074-b246-3dd6664c30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(df,variable):\n",
    "    df[variable+\"newvar\"]=np.where(df[variable].isnull(),\"Missing\",df[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98725b90-4db5-44c4-b4b4-4b65b0c2e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['BsmtQual','FireplaceQu','GarageType']:\n",
    "    impute_nan(df,feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90112883-30cf-4d92-8ef7-88f24fd8bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72871f72-718f-4bd1-8935-c301b0e53952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['BsmtQual','FireplaceQu','GarageType'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19039d9-2988-4ce3-a14f-bfc4ae657d3f",
   "metadata": {},
   "source": [
    "Handle Categorical Features\n",
    "*Nominal Encoding\n",
    "1. One Hot Encoding\n",
    "This is for nominal features.One Hot Encoding is a common way of preprocessing categorical features for machine learning models. This type of encoding creates a new binary feature for each possible category and assigns a value of 1 to the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a98a95-da87-402f-9936-80f4ec9fb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('titanic.csv',usecols=['Sex'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3b17d-adc8-4749-9c21-8c10d57ef488",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1691d7-298f-459c-ad7b-7a93243e5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here i am droping firt Sex_female feature cz Sex_male is carrying both of \n",
    "# information like 1 is male 0 if female\n",
    "pd.get_dummies(df,drop_first=True).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085610c-6304-4349-86dd-f9cff3a02e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('titanic.csv',usecols=['Embarked'])\n",
    "df['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45f874-3d44-4c9a-8562-eaf79f68d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "pd.get_dummies(df,drop_first=True).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b52b9-7180-452c-bb04-8dd55bbde374",
   "metadata": {},
   "source": [
    "Advantage and Disadvantage\n",
    "One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality.\n",
    "\n",
    "2. One hot encoding with many categories in a feature\n",
    "This is for nominal features. When we have a lots of category in our features ,then we have to use this method\n",
    "\n",
    "How to do?\n",
    "http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf In the winning solution of the KDD 2009 cup: \"Winning the KDD Cup Orange Challenge with Ensemble\n",
    "\n",
    "The Team suggested using 10 most frequent labels convert them into dummy variables using onehotencoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951cbea-b2c8-43b7-9996-4877b5a617f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('mercedes.csv',usecols=[\"X0\",\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\"])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba9e75-5df7-4e48-b27b-1d6560ab5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(len(df[i].unique()))\n",
    "df.X1.value_counts().sort_values(ascending=False).head(10)\n",
    "\n",
    "lst_10=df.X1.value_counts().sort_values(ascending=False).head(10).index\n",
    "lst_10=list(lst_10)\n",
    "lst_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd38332-24da-4030-b82c-cd2dbfc310a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for categories in lst_10:\n",
    "    df[categories]=np.where(df['X1']==categories,1,0)\n",
    "lst_10.append('X1')\n",
    "df[lst_10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383f7a6-a84c-4778-9e51-7f0a158557d5",
   "metadata": {},
   "source": [
    "3. Ordinal numbering encoding or Label Encoding\n",
    "Ordinal categorical variables\n",
    "\n",
    "Ordinal data is a categorical, statistical data type where the variables have natural, ordered categories and the distances between the categories is not known.\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "Student's grade in an exam (A, B, C or Fail).\n",
    "Educational level, with the categories: Elementary school, High school, College graduate, PhD ranked from 1 to 4.\n",
    "When the categorical variables are ordinal, the most straightforward best approach is to replace the labels by some ordinal number based on the ranks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f2995-330a-43a0-9122-0c42f99b2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today_date=datetime.datetime.today()\n",
    "today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab6ea0-fb31-4883-a694-503f9cbe4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date-datetime.timedelta(3)\n",
    "\n",
    "#List Comprehension\n",
    "days=[today_date-datetime.timedelta(x) for x in range(0,15)]\n",
    "import pandas as pd\n",
    "data=pd.DataFrame(days)\n",
    "data.columns=[\"Day\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d49d4-9505-49c9-881a-f103c7069a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekday']=data['Day'].dt.strftime('%A')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a3376-04c4-42a2-8750-4c21a489ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary\n",
    "data['weekday_ordinal']=data['weekday'].map(dictionary)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dfb4f4-faab-4d5a-9772-61b0b2afe5d3",
   "metadata": {},
   "source": [
    "Ordinal Measurement Advantages\n",
    "Ordinal measurement is normally used for surveys and questionnaires. Statistical analysis is applied to the responses once they are collected to place the people who took the survey into the various categories. The data is then compared to draw inferences and conclusions about the whole surveyed population with regard to the specific variables. The advantage of using ordinal measurement is ease of collation and categorization. If you ask a survey question without providing the variables, the answers are likely to be so diverse they cannot be converted to statistics.\n",
    "\n",
    "With Respect to Machine Learning\n",
    "\n",
    "Keeps the semantical information of the variable (human readable content)\n",
    "Straightforward\n",
    "Ordinal Measurement Disadvantages\n",
    "The same characteristics of ordinal measurement that create its advantages also create certain disadvantages. The responses are often so narrow in relation to the question that they create or magnify bias that is not factored into the survey. For example, on the question about satisfaction with the governor, people might be satisfied with his job performance but upset about a recent sex scandal. The survey question might lead respondents to state their dissatisfaction about the scandal, in spite of satisfaction with his job performance -- but the statistical conclusion will not differentiate.\n",
    "\n",
    "With Respect to Machine Learning\n",
    "\n",
    "Does not add machine learning valuable information\n",
    "4. Count Or Frequency Encoding (Nominal Features)\n",
    "High Cardinality\n",
    "\n",
    "Another way to refer to variables that have a multitude of categories, is to call them variables with high cardinality.\n",
    "\n",
    "If we have categorical variables containing many multiple labels or high cardinality,then by using one hot encoding, we will expand the feature space dramatically.\n",
    "\n",
    "One approach that is heavily used in Kaggle competitions, is to replace each label of the categorical variable by the count, this is the amount of times each label appears in the dataset. Or the frequency, this is the percentage of observations within that category. The 2 are equivalent.\n",
    "\n",
    "Let's see how this works:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66624e22-ab62-4845-8938-97c4862093e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data' , header = None,index_col=None)\n",
    "train_set.head()                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b7221-103a-4d39-ad68-f3804932315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in train_set.columns[:]:\n",
    "    print(feature,\":\",len(train_set[feature].unique()),'labels')\n",
    "country_map=train_set['Country'].value_counts().to_dict()\n",
    "train_set['Country']=train_set['Country'].map(country_map)\n",
    "train_set.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d88b63-7006-438a-845d-dcadf5dd4ab3",
   "metadata": {},
   "source": [
    "Advantages\n",
    "It is very simple to implement\n",
    "Does not increase the feature dimensional space\n",
    "Disadvantages\n",
    "If some of the labels have the same count, then they will be replaced with the same count and they will loose some valuable information.\n",
    "2 Adds somewhat arbitrary numbers, and therefore weights to the different labels, that may not be related to their predictive power\n",
    "\n",
    "Follow this thread in Kaggle for more information: https://www.kaggle.com/general/16927\n",
    "\n",
    "5. Target Guided Ordinal Encoding (Ordinal Features)\n",
    "Ordering the labels according to the target (Here survived feature is my target)\n",
    "Replace the labels by the joint probability of being 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96a765-52a5-42b3-aca0-41cac0853a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('titanic.csv', usecols=['Cabin','Survived'])\n",
    "df.head()\n",
    "\n",
    "df['Cabin'].fillna('Missing',inplace=True)\n",
    "df['Cabin']=df['Cabin'].astype(str).str[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae310d72-7f5d-44ba-99b8-02d7f144e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Cabin.unique()\n",
    "df.groupby(['Cabin'])['Survived'].mean()\n",
    "\n",
    "df.groupby(['Cabin'])['Survived'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690bb15c-3cd3-4609-a9ad-fb8b524a7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Cabin'])['Survived'].mean().sort_values().index\n",
    "\n",
    "ordinal_labels=df.groupby(['Cabin'])['Survived'].mean().sort_values().index\n",
    "ordinal_labels\n",
    "\n",
    "enumerate(ordinal_labels,0)\n",
    "\n",
    "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
    "ordinal_labels2\n",
    "\n",
    "df['Cabin_ordinal_labels']=df['Cabin'].map(ordinal_labels2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42e193-7a83-44f1-b3eb-1f3127f3c84a",
   "metadata": {},
   "source": [
    "6. Mean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2b3de-7809-49a6-82c8-be6de74d7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_ordinal=df.groupby(['Cabin'])['Survived'].mean().to_dict()\n",
    "mean_ordinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c63aad-398c-4e49-9a8f-f3b20bdb3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_ordinal_encode']=df['Cabin'].map(mean_ordinal)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6491c9d-f949-4491-8432-5c3b550ff901",
   "metadata": {},
   "source": [
    "Advantages\n",
    "It Captures information within the label,therefore it rendering more predictive features\n",
    "it creates some monotonic relationship between variable and the target\n",
    "Disadvantages\n",
    "It pronts to overfitting\n",
    "7.Probability Ratio Encoding\n",
    "steps:\n",
    "\n",
    "Probability of Survived based on Cabin--- Categorical Feature\n",
    "Probability of Not Survived---1-pr(Survived)\n",
    "pr(Survived)/pr(Not Survived)\n",
    "Dictonary to map cabin with probability\n",
    "replace with the categorical feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d363b-7e06-4577-bdf4-a0930c880206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"titanic.csv\",usecols=['Cabin','Survived'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7d6f6-5705-4e3c-9a92-512ba4482e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing Nan\n",
    "df['Cabin'].fillna('Missing',inplace=True)\n",
    "df.head()\n",
    "\n",
    "df['Cabin'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5f155-708f-42cf-a216-94a8378b165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin']=df['Cabin'].astype(str).str[0]\n",
    "df.head()\n",
    "\n",
    "df.Cabin.unique()\n",
    "\n",
    "prob_df = df.groupby(['Cabin'])['Survived'].mean()\n",
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e15bb-7f59-405f-8384-269c34af1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71419b-c98a-4f15-b593-832346b00aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[\"Died\"] = 1 - prob_df[\"Survived\"]\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b6ca4-9bcd-4f5e-ac3d-11fae103e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df['Probability_ratio']=prob_df['Survived'] / prob_df['Died']\n",
    "prob_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfee61c-e97c-4c01-86a2-b01ee43613e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_encoded=prob_df['Probability_ratio'].to_dict()\n",
    "probability_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70988c-35be-4d93-8142-e55681e71677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the nan by the probability ratio\n",
    "df['Cabin_encoded']=df['Cabin'].map(probability_encoded)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838921f-71de-4341-9dca-e48843446c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0749710-71cf-43a9-becd-fb3318d086b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96222561-7173-4bdf-85f4-b01b745faa47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
